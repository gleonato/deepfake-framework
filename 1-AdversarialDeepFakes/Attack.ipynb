{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.4.0\n",
      "  Downloading absl-py-0.4.0.tar.gz (88 kB)\n",
      "     |████████████████████████████████| 88 kB 4.1 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting astor==0.7.1\n",
      "  Downloading astor-0.7.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting certifi==2018.11.29\n",
      "  Downloading certifi-2018.11.29-py2.py3-none-any.whl (154 kB)\n",
      "     |████████████████████████████████| 154 kB 22.7 MB/s            \n",
      "\u001b[?25hCollecting cffi==1.12.1\n",
      "  Downloading cffi-1.12.1-cp36-cp36m-manylinux1_x86_64.whl (428 kB)\n",
      "     |████████████████████████████████| 428 kB 59.8 MB/s            \n",
      "\u001b[?25hCollecting cmake==3.12.0\n",
      "  Downloading cmake-3.12.0-cp36-cp36m-manylinux1_x86_64.whl (17.7 MB)\n",
      "     |████████████████████████████████| 17.7 MB 99.2 MB/s            \n",
      "\u001b[?25hCollecting dlib==19.15.0\n",
      "  Downloading dlib-19.15.0.tar.gz (3.3 MB)\n",
      "     |████████████████████████████████| 3.3 MB 86.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting face-recognition==1.2.3\n",
      "  Downloading face_recognition-1.2.3-py2.py3-none-any.whl (21 kB)\n",
      "Collecting face-recognition-models==0.3.0\n",
      "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
      "     |████████████████████████████████| 100.1 MB 21 kB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ffmpy==0.2.2\n",
      "  Downloading ffmpy-0.2.2.tar.gz (4.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gast==0.2.0\n",
      "  Downloading gast-0.2.0.tar.gz (9.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grpcio==1.14.1\n",
      "  Downloading grpcio-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (9.3 MB)\n",
      "     |████████████████████████████████| 9.3 MB 106.5 MB/s            \n",
      "\u001b[?25hCollecting h5py==2.8.0\n",
      "  Downloading h5py-2.8.0-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
      "     |████████████████████████████████| 2.8 MB 71.6 MB/s            \n",
      "\u001b[?25hCollecting Keras==2.2.0\n",
      "  Downloading Keras-2.2.0-py2.py3-none-any.whl (300 kB)\n",
      "     |████████████████████████████████| 300 kB 112.4 MB/s            \n",
      "\u001b[?25hCollecting Markdown==2.6.11\n",
      "  Downloading Markdown-2.6.11-py2.py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 2.1 MB/s              \n",
      "\u001b[?25hRequirement already satisfied: mkl-fft in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: mkl-random in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirements.txt (line 16)) (1.2.0)\n",
      "Collecting munch==2.3.2\n",
      "  Downloading munch-2.3.2.tar.gz (7.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy==1.16.2\n",
      "  Downloading numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3 MB)\n",
      "     |████████████████████████████████| 17.3 MB 43.7 MB/s            \n",
      "\u001b[?25hCollecting nvidia-ml-py3==7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: olefile==0.46 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirements.txt (line 20)) (0.46)\n",
      "Collecting opencv-python==3.4.1.15\n",
      "  Downloading opencv_python-3.4.1.15-cp36-cp36m-manylinux1_x86_64.whl (24.9 MB)\n",
      "     |████████████████████████████████| 24.9 MB 45.8 MB/s            \n",
      "\u001b[?25hCollecting pathlib==1.0.1\n",
      "  Downloading pathlib-1.0.1.tar.gz (49 kB)\n",
      "     |████████████████████████████████| 49 kB 11.5 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Pillow==5.4.1\n",
      "  Downloading Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 66.3 MB/s            \n",
      "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 1.7 MB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting protobuf==3.6.1\n",
      "  Downloading protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 50.0 MB/s            \n",
      "\u001b[?25hCollecting pycparser==2.19\n",
      "  Downloading pycparser-2.19.tar.gz (158 kB)\n",
      "     |████████████████████████████████| 158 kB 127.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scandir==1.7\n",
      "  Downloading scandir-1.7.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting six==1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch==1.0.1.post2\n",
      "  Downloading torch-1.0.1.post2-cp36-cp36m-manylinux1_x86_64.whl (582.5 MB)\n",
      "     |██████████████████              | 328.3 MB 98.3 MB/s eta 0:00:03 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |████████████████████████████████| 582.5 MB 16 kB/s               \n",
      "\u001b[?25hCollecting torchvision==0.2.1\n",
      "  Downloading torchvision-0.2.1-py2.py3-none-any.whl (54 kB)\n",
      "     |████████████████████████████████| 54 kB 4.1 MB/s             \n",
      "\u001b[?25hCollecting tqdm==4.25.0\n",
      "  Downloading tqdm-4.25.0-py2.py3-none-any.whl (43 kB)\n",
      "     |████████████████████████████████| 43 kB 384 kB/s             \n",
      "\u001b[?25hCollecting torchgeometry==0.1.2\n",
      "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 112 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: Click>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from face-recognition==1.2.3->-r requirements.txt (line 7)) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from Keras==2.2.0->-r requirements.txt (line 13)) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from Keras==2.2.0->-r requirements.txt (line 13)) (1.5.3)\n",
      "Collecting keras-preprocessing==1.0.1\n",
      "  Downloading Keras_Preprocessing-1.0.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras-applications==1.0.2\n",
      "  Downloading Keras_Applications-1.0.2-py2.py3-none-any.whl (43 kB)\n",
      "     |████████████████████████████████| 43 kB 139 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from protobuf==3.6.1->-r requirements.txt (line 25)) (49.6.0.post20210108)\n",
      "Building wheels for collected packages: absl-py, dlib, face-recognition-models, ffmpy, gast, munch, nvidia-ml-py3, pathlib, pretrainedmodels, pycparser, scandir, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.4.0-py3-none-any.whl size=106042 sha256=af5a64a7c7fba11ec3775657019eddd5196a5d85ee6dc25097f2b89b4beb1dd9\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/89/01/54/e8bbe08f43d1f56f6e6081eb279b4b2f83c3663dd8f2c48521\n",
      "  Building wheel for dlib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dlib: filename=dlib-19.15.0-cp36-cp36m-linux_x86_64.whl size=3944469 sha256=49a9fa5b815e346701fe65bf8c03afdfebef51b114177e4522d7ec3098e91166\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4d/94/20/79c6c9d3fd2ae697cc133d33810fb225d474ea6d848160ce61\n",
      "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=fb342ca8a1a87d4af5144de4bda982fcc78523b5c9ff2edd560ec846407f2bed\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6a/e1/1a/8969952b51c25409d5b96ecb09603de12b8534bd6d68e6e7d1\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.2.2-py3-none-any.whl size=4600 sha256=606ff526c6a07a6aafaf4704ce37298ab33ec6511ddda534aed8ea82a796c9fb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f9/d1/7b/f18caf5d24d29e673800156ebcf2462fefa272f7c5059afce6\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.0-py3-none-any.whl size=6664 sha256=63887f1166e541a9b2014d497f9b148ea777326074620f2569024cd703ac7b86\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/b0/47/8b/196628a5926beb2b8c2f50c2050b18911a838bbf38e9532e67\n",
      "  Building wheel for munch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for munch: filename=munch-2.3.2-py2.py3-none-any.whl size=6613 sha256=6b4b6647161f50be8fb4d0762aa4526fb9a0472af846c3489fab1d5c74dfd5a2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/83/2a/b6/7ba9a08c2d20b71a4360023dd82c27a89b90b421fb6cda69eb\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=467dc13bc6aea590bd70a55036eae5320c05578e6548fea6d8d23389c33c4130\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/7f/26/a3/33f2079871e2bebb3f53a2b21c3ec64129b8efdd18a6263a52\n",
      "  Building wheel for pathlib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathlib: filename=pathlib-1.0.1-py3-none-any.whl size=14347 sha256=369482993a1efb67566c9ca1e51122b365cbb840c850ee2cbc404ff3f7017641\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e1/32/91/afe2cabe6f77819de11759f2a07d538cd521ef3a9dd81ba0b4\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60963 sha256=e865eb78c9275a4d3c14d75d32d45690b1379ee0d4b4639ccf85483ffc923b08\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/9c/03/81/494596f1d017bbf80daf53bc33dbb05aaa5a6ad6d44c74011f\n",
      "  Building wheel for pycparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111031 sha256=83b45fc313276335010ede6687ccb3516d1d0edfebf90aafeaf1ec1351579024\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c6/6b/83/2608afaa57ecfb0a66ac89191a8d9bad71c62ca55ee499c2d0\n",
      "  Building wheel for scandir (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scandir: filename=scandir-1.7-cp36-cp36m-linux_x86_64.whl size=20146 sha256=6b6302bc11de18ade39d053c98717c2b6ab958fd45b7edb929fac835926bf1c8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6c/55/58/b4d6139e36f2d0227c60c5c8cd083411a4602ad5945efaa9ac\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=016d0ad74357fa14eadd1fa00ea0432bfd1ad3bdc2e80b5a2661605bb3bc52eb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built absl-py dlib face-recognition-models ffmpy gast munch nvidia-ml-py3 pathlib pretrainedmodels pycparser scandir termcolor\n",
      "Installing collected packages: six, numpy, torch, Pillow, h5py, tqdm, torchvision, pycparser, munch, keras-preprocessing, keras-applications, face-recognition-models, dlib, torchgeometry, termcolor, scandir, protobuf, pretrainedmodels, pathlib, opencv-python, nvidia-ml-py3, Markdown, Keras, grpcio, gast, ffmpy, face-recognition, cmake, cffi, certifi, astor, absl-py\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.1\n",
      "    Uninstalling torch-1.7.1:\n",
      "      Successfully uninstalled torch-1.7.1\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.4.0\n",
      "    Uninstalling Pillow-8.4.0:\n",
      "      Successfully uninstalled Pillow-8.4.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.8.2\n",
      "    Uninstalling torchvision-0.8.2:\n",
      "      Successfully uninstalled torchvision-0.8.2\n",
      "  Attempting uninstall: pycparser\n",
      "    Found existing installation: pycparser 2.20\n",
      "    Uninstalling pycparser-2.20:\n",
      "      Successfully uninstalled pycparser-2.20\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.5.1.48\n",
      "    Uninstalling opencv-python-4.5.1.48:\n",
      "      Successfully uninstalled opencv-python-4.5.1.48\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.14.5\n",
      "    Uninstalling cffi-1.14.5:\n",
      "      Successfully uninstalled cffi-1.14.5\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2021.5.30\n",
      "    Uninstalling certifi-2021.5.30:\n",
      "      Successfully uninstalled certifi-2021.5.30\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n",
      "shap 0.40.0 requires tqdm>4.25.0, but you have tqdm 4.25.0 which is incompatible.\n",
      "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.2 which is incompatible.\n",
      "onnx 1.10.0 requires numpy>=1.16.6, but you have numpy 1.16.2 which is incompatible.\n",
      "matplotlib 3.3.4 requires pillow>=6.2.0, but you have pillow 5.4.1 which is incompatible.\n",
      "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 5.4.1 which is incompatible.\n",
      "aiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.23.46 which is incompatible.\u001b[0m\n",
      "Successfully installed Keras-2.2.0 Markdown-2.6.11 Pillow-5.4.1 absl-py-0.4.0 astor-0.7.1 certifi-2018.11.29 cffi-1.12.1 cmake-3.12.0 dlib-19.15.0 face-recognition-1.2.3 face-recognition-models-0.3.0 ffmpy-0.2.2 gast-0.2.0 grpcio-1.14.1 h5py-2.8.0 keras-applications-1.0.2 keras-preprocessing-1.0.1 munch-2.3.2 numpy-1.16.2 nvidia-ml-py3-7.352.0 opencv-python-3.4.1.15 pathlib-1.0.1 pretrainedmodels-0.7.4 protobuf-3.6.1 pycparser-2.19 scandir-1.7 six-1.12.0 termcolor-1.1.0 torch-1.0.1.post2 torchgeometry-0.1.2 torchvision-0.2.1 tqdm-4.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create adversarial videos that can fool xceptionnet.\n",
    "\n",
    "Usage:\n",
    "python attack.py\n",
    "    -i <folder with video files or path to video file>\n",
    "    -m <path to model file>\n",
    "    -o <path to output folder, will write one or multiple output videos there>\n",
    "\n",
    "built upon the code by Andreas Rössler for detecting deep fakes.\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import argparse\n",
    "from os.path import join\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from network.models import model_selection\n",
    "from dataset.transform import xception_default_data_transforms, mesonet_default_data_transforms\n",
    "from torch import autograd\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "import attack_algos\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
    "    \"\"\"\n",
    "    Expects a dlib face to generate a quadratic bounding box.\n",
    "    :param face: dlib face class\n",
    "    :param width: frame width\n",
    "    :param height: frame height\n",
    "    :param scale: bounding box size multiplier to get a bigger face region\n",
    "    :param minsize: set minimum bounding box size\n",
    "    :return: x, y, bounding_box_size in opencv form\n",
    "    \"\"\"\n",
    "    x1 = face.left()\n",
    "    y1 = face.top()\n",
    "    x2 = face.right()\n",
    "    y2 = face.bottom()\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
    "    if minsize:\n",
    "        if size_bb < minsize:\n",
    "            size_bb = minsize\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "    # Check for out of bounds, x-y top left corner\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    # Check for too big bb size for given x, y\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "\n",
    "    return x1, y1, size_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessamento\n",
    "\n",
    "def preprocess_image(image, model_type, cuda=True, legacy = False):\n",
    "    \"\"\"\n",
    "    Preprocesses the image such that it can be fed into our network.\n",
    "    During this process we envoke PIL to cast it into a PIL image.\n",
    "\n",
    "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
    "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
    "    necessarily casted to cuda\n",
    "    \"\"\"\n",
    "    # Revert from BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Preprocess using the preprocessing function used during training and\n",
    "    # casting it to PIL image\n",
    "    if not legacy:\n",
    "        # only conver to tensor here, \n",
    "        # other transforms -> resize, normalize differentiable done in predict_from_model func\n",
    "        # same for meso, xception\n",
    "        preprocess = xception_default_data_transforms['to_tensor']\n",
    "    else:\n",
    "        if model_type == \"xception\":\n",
    "            preprocess = xception_default_data_transforms['test']\n",
    "        elif model_type == \"meso\":\n",
    "            preprocess = mesonet_default_data_transforms['test']\n",
    "\n",
    "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
    "    \n",
    "    # Add first dimension as the network expects a batch\n",
    "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
    "    if cuda:\n",
    "        preprocessed_image = preprocessed_image.cuda()\n",
    "\n",
    "    preprocessed_image.requires_grad = True\n",
    "    return preprocessed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un_preprocess_image(image, size):\n",
    "    \"\"\"\n",
    "    Tensor to PIL image and RGB to BGR\n",
    "    \"\"\"\n",
    "    \n",
    "    image.detach()\n",
    "    new_image = image.squeeze(0)\n",
    "    new_image = new_image.detach().cpu()\n",
    "\n",
    "    undo_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    new_image = undo_transform(new_image)\n",
    "    new_image = numpy.array(new_image)\n",
    "\n",
    "    new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return new_image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model_legacy(image, model, model_type, post_function=nn.Softmax(dim=1),\n",
    "                       cuda=True):\n",
    "    \"\"\"\n",
    "    Predicts the label of an input image. Preprocesses the input image and\n",
    "    casts it to cuda if required\n",
    "\n",
    "    :param image: numpy image\n",
    "    :param model: torch model with linear layer at the end\n",
    "    :param post_function: e.g., softmax\n",
    "    :param cuda: enables cuda, must be the same parameter as the model\n",
    "    :return: prediction (1 = fake, 0 = real)\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    preprocessed_image = preprocess_image(image, model_type, cuda, legacy = True)\n",
    "\n",
    "    # Model prediction\n",
    "    output = model(preprocessed_image)\n",
    "    output = post_function(output)\n",
    "\n",
    "    # Cast to desired\n",
    "    _, prediction = torch.max(output, 1)    # argmax\n",
    "    prediction = float(prediction.cpu().numpy())\n",
    "\n",
    "    return int(prediction), output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: /home/ec2-user/SageMaker/deepfake-framework/Dataset/25/manipulated_sequences/Deepfakes/raw/videos/183_253.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/390 [00:00<01:02,  6.20it/s]\u001b[A\n",
      "  1%|          | 2/390 [00:00<01:43,  3.73it/s]\u001b[A\n",
      "  1%|          | 3/390 [00:01<02:06,  3.05it/s]\u001b[A\n",
      "  1%|          | 4/390 [00:01<02:20,  2.75it/s]\u001b[A\n",
      "  1%|▏         | 5/390 [00:02<02:30,  2.56it/s]\u001b[A\n",
      "  2%|▏         | 6/390 [00:02<02:35,  2.46it/s]\u001b[A\n",
      "  2%|▏         | 7/390 [00:02<02:40,  2.39it/s]\u001b[A\n",
      "  2%|▏         | 8/390 [00:03<02:42,  2.35it/s]\u001b[A\n",
      "  2%|▏         | 9/390 [00:03<02:44,  2.32it/s]\u001b[A\n",
      "  3%|▎         | 10/390 [00:04<02:45,  2.30it/s]\u001b[A\n",
      "  3%|▎         | 11/390 [00:04<02:46,  2.28it/s]\u001b[A\n",
      "  3%|▎         | 12/390 [00:05<02:45,  2.28it/s]\u001b[A\n",
      "  3%|▎         | 13/390 [00:05<02:45,  2.27it/s]\u001b[A\n",
      "  4%|▎         | 14/390 [00:06<02:45,  2.28it/s]\u001b[A\n",
      "  4%|▍         | 15/390 [00:06<02:48,  2.22it/s]\u001b[A\n",
      "  4%|▍         | 16/390 [00:07<03:00,  2.07it/s]\u001b[A\n",
      "  4%|▍         | 17/390 [00:07<02:55,  2.12it/s]\u001b[A\n",
      "  5%|▍         | 18/390 [00:08<02:59,  2.07it/s]\u001b[A\n",
      "  5%|▍         | 19/390 [00:08<02:55,  2.12it/s]\u001b[A\n",
      "  5%|▌         | 20/390 [00:08<02:51,  2.16it/s]\u001b[A\n",
      "  5%|▌         | 21/390 [00:09<02:49,  2.18it/s]\u001b[A\n",
      "  6%|▌         | 22/390 [00:09<02:47,  2.20it/s]\u001b[A\n",
      "  6%|▌         | 23/390 [00:10<02:47,  2.20it/s]\u001b[A\n",
      "  6%|▌         | 24/390 [00:10<02:48,  2.18it/s]\u001b[A\n",
      "  6%|▋         | 25/390 [00:11<02:47,  2.18it/s]\u001b[A\n",
      "  7%|▋         | 26/390 [00:11<02:45,  2.20it/s]\u001b[A\n",
      "  7%|▋         | 27/390 [00:12<02:52,  2.10it/s]\u001b[A\n",
      "  7%|▋         | 28/390 [00:12<03:04,  1.97it/s]\u001b[A\n",
      "  7%|▋         | 29/390 [00:13<02:59,  2.02it/s]\u001b[A\n",
      "  8%|▊         | 30/390 [00:13<02:53,  2.08it/s]\u001b[A\n",
      "  8%|▊         | 31/390 [00:14<02:56,  2.03it/s]\u001b[A\n",
      "  8%|▊         | 32/390 [00:14<02:51,  2.09it/s]\u001b[A\n",
      "  8%|▊         | 33/390 [00:15<02:59,  1.99it/s]\u001b[A\n",
      "  9%|▊         | 34/390 [00:15<02:53,  2.05it/s]\u001b[A\n",
      "  9%|▉         | 35/390 [00:16<02:50,  2.09it/s]\u001b[A\n",
      "  9%|▉         | 36/390 [00:16<02:48,  2.10it/s]\u001b[A\n",
      "  9%|▉         | 37/390 [00:17<02:46,  2.12it/s]\u001b[A\n",
      " 10%|▉         | 38/390 [00:17<02:41,  2.18it/s]\u001b[A\n",
      " 10%|█         | 39/390 [00:17<02:49,  2.07it/s]\u001b[A\n",
      " 10%|█         | 40/390 [00:18<02:44,  2.13it/s]\u001b[A\n",
      " 11%|█         | 41/390 [00:18<02:48,  2.08it/s]\u001b[A\n",
      " 11%|█         | 42/390 [00:19<02:42,  2.14it/s]\u001b[A\n",
      " 11%|█         | 43/390 [00:19<02:51,  2.03it/s]\u001b[A\n",
      " 11%|█▏        | 44/390 [00:20<02:46,  2.07it/s]\u001b[A\n",
      " 12%|█▏        | 45/390 [00:20<02:42,  2.13it/s]\u001b[A\n",
      " 12%|█▏        | 46/390 [00:21<02:40,  2.15it/s]\u001b[A\n",
      " 12%|█▏        | 47/390 [00:21<02:37,  2.18it/s]\u001b[A\n",
      " 12%|█▏        | 48/390 [00:22<02:35,  2.20it/s]\u001b[A\n",
      " 13%|█▎        | 49/390 [00:22<02:33,  2.22it/s]\u001b[A\n",
      " 13%|█▎        | 50/390 [00:23<02:30,  2.25it/s]\u001b[A\n",
      " 13%|█▎        | 51/390 [00:23<02:42,  2.09it/s]\u001b[A\n",
      " 13%|█▎        | 52/390 [00:24<02:37,  2.14it/s]\u001b[A\n",
      " 14%|█▎        | 53/390 [00:24<02:33,  2.19it/s]\u001b[A\n",
      " 14%|█▍        | 54/390 [00:24<02:37,  2.13it/s]\u001b[A\n",
      " 14%|█▍        | 55/390 [00:25<02:33,  2.18it/s]\u001b[A\n",
      " 14%|█▍        | 56/390 [00:25<02:31,  2.21it/s]\u001b[A\n",
      " 15%|█▍        | 57/390 [00:26<02:28,  2.25it/s]\u001b[A\n",
      " 15%|█▍        | 58/390 [00:26<02:26,  2.27it/s]\u001b[A\n",
      " 15%|█▌        | 59/390 [00:27<02:33,  2.15it/s]\u001b[A\n",
      " 15%|█▌        | 60/390 [00:27<02:30,  2.19it/s]\u001b[A\n",
      " 16%|█▌        | 61/390 [00:28<02:37,  2.09it/s]\u001b[A\n",
      " 16%|█▌        | 62/390 [00:28<02:33,  2.13it/s]\u001b[A\n",
      " 16%|█▌        | 63/390 [00:29<02:39,  2.05it/s]\u001b[A\n",
      " 16%|█▋        | 64/390 [00:29<02:33,  2.13it/s]\u001b[A\n",
      " 17%|█▋        | 65/390 [00:30<02:28,  2.19it/s]\u001b[A\n",
      " 17%|█▋        | 66/390 [00:31<04:38,  1.16it/s]\u001b[A\n",
      " 17%|█▋        | 67/390 [00:32<03:59,  1.35it/s]\u001b[A\n",
      " 17%|█▋        | 68/390 [00:32<03:29,  1.54it/s]\u001b[A\n",
      " 18%|█▊        | 69/390 [00:33<03:18,  1.62it/s]\u001b[A\n",
      " 18%|█▊        | 70/390 [00:33<03:00,  1.77it/s]\u001b[A\n",
      " 18%|█▊        | 71/390 [00:34<02:47,  1.91it/s]\u001b[A\n",
      " 18%|█▊        | 72/390 [00:34<02:37,  2.02it/s]\u001b[A\n",
      " 19%|█▊        | 73/390 [00:34<02:31,  2.09it/s]\u001b[A\n",
      " 19%|█▉        | 74/390 [00:35<02:27,  2.15it/s]\u001b[A\n",
      " 19%|█▉        | 75/390 [00:35<02:23,  2.19it/s]\u001b[A\n",
      " 19%|█▉        | 76/390 [00:36<02:21,  2.22it/s]\u001b[A\n",
      " 20%|█▉        | 77/390 [00:36<02:19,  2.25it/s]\u001b[A\n",
      " 20%|██        | 78/390 [00:37<02:17,  2.27it/s]\u001b[A\n",
      " 20%|██        | 79/390 [00:37<02:26,  2.13it/s]\u001b[A\n",
      " 21%|██        | 80/390 [00:38<02:24,  2.14it/s]\u001b[A\n",
      " 21%|██        | 81/390 [00:38<02:24,  2.14it/s]\u001b[A\n",
      " 21%|██        | 82/390 [00:39<02:23,  2.15it/s]\u001b[A\n",
      " 21%|██▏       | 83/390 [00:39<02:19,  2.20it/s]\u001b[A\n",
      " 22%|██▏       | 84/390 [00:39<02:16,  2.24it/s]\u001b[A\n",
      " 22%|██▏       | 85/390 [00:40<02:15,  2.25it/s]\u001b[A\n",
      " 22%|██▏       | 86/390 [00:40<02:13,  2.27it/s]\u001b[A\n",
      " 22%|██▏       | 87/390 [00:41<02:12,  2.28it/s]\u001b[A\n",
      " 23%|██▎       | 88/390 [00:41<02:11,  2.29it/s]\u001b[A\n",
      " 23%|██▎       | 89/390 [00:42<02:16,  2.20it/s]\u001b[A\n",
      " 23%|██▎       | 90/390 [00:42<02:14,  2.24it/s]\u001b[A\n",
      " 23%|██▎       | 91/390 [00:43<02:12,  2.26it/s]\u001b[A\n",
      " 24%|██▎       | 92/390 [00:43<02:11,  2.26it/s]\u001b[A\n",
      " 24%|██▍       | 93/390 [00:43<02:10,  2.27it/s]\u001b[A\n",
      " 24%|██▍       | 94/390 [00:44<02:09,  2.29it/s]\u001b[A\n",
      " 24%|██▍       | 95/390 [00:44<02:08,  2.30it/s]\u001b[A\n",
      " 25%|██▍       | 96/390 [00:45<02:07,  2.31it/s]\u001b[A\n",
      " 25%|██▍       | 97/390 [00:45<02:06,  2.31it/s]\u001b[A\n",
      " 25%|██▌       | 98/390 [00:46<02:06,  2.32it/s]\u001b[A\n",
      " 25%|██▌       | 99/390 [00:46<02:05,  2.31it/s]\u001b[A\n",
      " 26%|██▌       | 100/390 [00:46<02:04,  2.32it/s]\u001b[A\n",
      " 26%|██▌       | 101/390 [00:47<02:04,  2.32it/s]\u001b[A\n",
      " 26%|██▌       | 102/390 [00:47<02:04,  2.31it/s]\u001b[A\n",
      " 26%|██▋       | 103/390 [00:48<02:04,  2.31it/s]\u001b[A\n",
      " 27%|██▋       | 104/390 [00:48<02:12,  2.16it/s]\u001b[A\n",
      " 27%|██▋       | 105/390 [00:49<02:09,  2.20it/s]\u001b[A\n",
      " 27%|██▋       | 106/390 [00:49<02:16,  2.08it/s]\u001b[A\n",
      " 27%|██▋       | 107/390 [00:50<02:14,  2.10it/s]\u001b[A\n",
      " 28%|██▊       | 108/390 [00:50<02:18,  2.03it/s]\u001b[A\n",
      " 28%|██▊       | 109/390 [00:51<02:14,  2.09it/s]\u001b[A\n",
      " 28%|██▊       | 110/390 [00:51<02:17,  2.03it/s]\u001b[A\n",
      " 28%|██▊       | 111/390 [00:52<02:11,  2.11it/s]\u001b[A\n",
      " 29%|██▊       | 112/390 [00:52<02:16,  2.03it/s]\u001b[A\n",
      " 29%|██▉       | 113/390 [00:53<02:12,  2.09it/s]\u001b[A\n",
      " 29%|██▉       | 114/390 [00:53<02:08,  2.15it/s]\u001b[A\n",
      " 29%|██▉       | 115/390 [00:53<02:04,  2.21it/s]\u001b[A\n",
      " 30%|██▉       | 116/390 [00:54<02:12,  2.07it/s]\u001b[A\n",
      " 30%|███       | 117/390 [00:54<02:08,  2.13it/s]\u001b[A\n",
      " 30%|███       | 118/390 [00:55<02:14,  2.02it/s]\u001b[A\n",
      " 31%|███       | 119/390 [00:55<02:10,  2.07it/s]\u001b[A\n",
      " 31%|███       | 120/390 [00:56<02:06,  2.13it/s]\u001b[A\n",
      " 31%|███       | 121/390 [00:56<02:03,  2.18it/s]\u001b[A\n",
      " 31%|███▏      | 122/390 [00:57<02:00,  2.22it/s]\u001b[A\n",
      " 32%|███▏      | 123/390 [00:57<01:58,  2.25it/s]\u001b[A\n",
      " 32%|███▏      | 124/390 [00:58<01:57,  2.26it/s]\u001b[A\n",
      " 32%|███▏      | 125/390 [00:58<01:56,  2.27it/s]\u001b[A\n",
      " 32%|███▏      | 126/390 [00:59<01:55,  2.28it/s]\u001b[A\n",
      " 33%|███▎      | 127/390 [00:59<02:02,  2.14it/s]\u001b[A\n",
      " 33%|███▎      | 128/390 [00:59<01:59,  2.20it/s]\u001b[A\n",
      " 33%|███▎      | 129/390 [01:00<02:03,  2.11it/s]\u001b[A\n",
      " 33%|███▎      | 130/390 [01:00<02:01,  2.14it/s]\u001b[A\n",
      " 34%|███▎      | 131/390 [01:01<02:07,  2.04it/s]\u001b[A\n",
      " 34%|███▍      | 132/390 [01:01<02:02,  2.11it/s]\u001b[A\n",
      " 34%|███▍      | 133/390 [01:02<02:04,  2.07it/s]\u001b[A\n",
      " 34%|███▍      | 134/390 [01:02<02:00,  2.13it/s]\u001b[A\n",
      " 35%|███▍      | 135/390 [01:03<01:56,  2.18it/s]\u001b[A\n",
      " 35%|███▍      | 136/390 [01:03<01:54,  2.22it/s]\u001b[A\n",
      " 35%|███▌      | 137/390 [01:04<01:52,  2.25it/s]\u001b[A\n",
      " 35%|███▌      | 138/390 [01:04<01:51,  2.26it/s]\u001b[A\n",
      " 36%|███▌      | 139/390 [01:05<01:50,  2.27it/s]\u001b[A\n",
      " 36%|███▌      | 140/390 [01:05<01:48,  2.30it/s]\u001b[A\n",
      " 36%|███▌      | 141/390 [01:05<01:47,  2.31it/s]\u001b[A\n",
      " 36%|███▋      | 142/390 [01:06<01:47,  2.31it/s]\u001b[A\n",
      " 37%|███▋      | 143/390 [01:06<01:47,  2.31it/s]\u001b[A\n",
      " 37%|███▋      | 144/390 [01:07<01:46,  2.30it/s]\u001b[A\n",
      " 37%|███▋      | 145/390 [01:07<01:47,  2.28it/s]\u001b[A\n",
      " 37%|███▋      | 146/390 [01:08<01:46,  2.28it/s]\u001b[A\n",
      " 38%|███▊      | 147/390 [01:08<01:45,  2.30it/s]\u001b[A\n",
      " 38%|███▊      | 148/390 [01:08<01:44,  2.31it/s]\u001b[A\n",
      " 38%|███▊      | 149/390 [01:09<01:50,  2.18it/s]\u001b[A\n",
      " 38%|███▊      | 150/390 [01:09<01:47,  2.23it/s]\u001b[A\n",
      " 39%|███▊      | 151/390 [01:10<01:46,  2.24it/s]\u001b[A\n",
      " 39%|███▉      | 152/390 [01:10<01:46,  2.24it/s]\u001b[A\n",
      " 39%|███▉      | 153/390 [01:11<01:46,  2.23it/s]\u001b[A\n",
      " 39%|███▉      | 154/390 [01:11<01:45,  2.24it/s]\u001b[A\n",
      " 40%|███▉      | 155/390 [01:12<01:49,  2.15it/s]\u001b[A\n",
      " 40%|████      | 156/390 [01:12<01:46,  2.20it/s]\u001b[A\n",
      " 40%|████      | 157/390 [01:13<01:50,  2.12it/s]\u001b[A\n",
      " 41%|████      | 158/390 [01:13<01:46,  2.18it/s]\u001b[A\n",
      " 41%|████      | 159/390 [01:13<01:45,  2.19it/s]\u001b[A\n",
      " 41%|████      | 160/390 [01:14<01:45,  2.17it/s]\u001b[A\n",
      " 41%|████▏     | 161/390 [01:14<01:45,  2.17it/s]\u001b[A\n",
      " 42%|████▏     | 162/390 [01:15<01:43,  2.19it/s]\u001b[A\n",
      " 42%|████▏     | 163/390 [01:15<01:48,  2.09it/s]\u001b[A\n",
      " 42%|████▏     | 164/390 [01:16<01:45,  2.15it/s]\u001b[A\n",
      " 42%|████▏     | 165/390 [01:16<01:42,  2.19it/s]\u001b[A\n",
      " 43%|████▎     | 166/390 [01:17<01:41,  2.21it/s]\u001b[A\n",
      " 43%|████▎     | 167/390 [01:17<01:41,  2.20it/s]\u001b[A\n",
      " 43%|████▎     | 168/390 [01:18<01:38,  2.25it/s]\u001b[A\n",
      " 43%|████▎     | 169/390 [01:18<01:47,  2.05it/s]\u001b[A\n",
      " 44%|████▎     | 170/390 [01:19<01:43,  2.12it/s]\u001b[A\n",
      " 44%|████▍     | 171/390 [01:19<01:40,  2.18it/s]\u001b[A\n",
      " 44%|████▍     | 172/390 [01:19<01:38,  2.22it/s]\u001b[A\n",
      " 44%|████▍     | 173/390 [01:20<01:36,  2.25it/s]\u001b[A\n",
      " 45%|████▍     | 174/390 [01:20<01:34,  2.28it/s]\u001b[A\n",
      " 45%|████▍     | 175/390 [01:21<01:34,  2.28it/s]\u001b[A\n",
      " 45%|████▌     | 176/390 [01:21<01:33,  2.30it/s]\u001b[A\n",
      " 45%|████▌     | 177/390 [01:22<01:31,  2.32it/s]\u001b[A\n",
      " 46%|████▌     | 178/390 [01:22<01:36,  2.19it/s]\u001b[A\n",
      " 46%|████▌     | 179/390 [01:23<01:34,  2.23it/s]\u001b[A\n",
      " 46%|████▌     | 180/390 [01:23<01:32,  2.27it/s]\u001b[A\n",
      " 46%|████▋     | 181/390 [01:23<01:31,  2.29it/s]\u001b[A\n",
      " 47%|████▋     | 182/390 [01:24<01:30,  2.31it/s]\u001b[A\n",
      " 47%|████▋     | 183/390 [01:24<01:29,  2.30it/s]\u001b[A\n",
      " 47%|████▋     | 184/390 [01:25<01:29,  2.30it/s]\u001b[A\n",
      " 47%|████▋     | 185/390 [01:25<01:28,  2.31it/s]\u001b[A\n",
      " 48%|████▊     | 186/390 [01:26<01:28,  2.32it/s]\u001b[A\n",
      " 48%|████▊     | 187/390 [01:26<01:27,  2.32it/s]\u001b[A\n",
      " 48%|████▊     | 188/390 [01:26<01:27,  2.31it/s]\u001b[A\n",
      " 48%|████▊     | 189/390 [01:27<01:26,  2.32it/s]\u001b[A\n",
      " 49%|████▊     | 190/390 [01:27<01:26,  2.32it/s]\u001b[A\n",
      " 49%|████▉     | 191/390 [01:28<01:25,  2.32it/s]\u001b[A\n",
      " 49%|████▉     | 192/390 [01:28<01:25,  2.32it/s]\u001b[A\n",
      " 49%|████▉     | 193/390 [01:29<01:26,  2.27it/s]\u001b[A\n",
      " 50%|████▉     | 194/390 [01:29<01:26,  2.28it/s]\u001b[A\n",
      " 50%|█████     | 195/390 [01:29<01:25,  2.28it/s]\u001b[A\n",
      " 50%|█████     | 196/390 [01:30<01:25,  2.28it/s]\u001b[A\n",
      " 51%|█████     | 197/390 [01:30<01:24,  2.28it/s]\u001b[A\n",
      " 51%|█████     | 198/390 [01:31<01:23,  2.29it/s]\u001b[A\n",
      " 51%|█████     | 199/390 [01:31<01:23,  2.29it/s]\u001b[A\n",
      " 51%|█████▏    | 200/390 [01:32<01:26,  2.19it/s]\u001b[A\n",
      " 52%|█████▏    | 201/390 [01:32<01:24,  2.23it/s]\u001b[A\n",
      " 52%|█████▏    | 202/390 [01:33<01:27,  2.14it/s]\u001b[A\n",
      " 52%|█████▏    | 203/390 [01:33<01:25,  2.20it/s]\u001b[A\n",
      " 52%|█████▏    | 204/390 [01:34<01:26,  2.14it/s]\u001b[A\n",
      " 53%|█████▎    | 205/390 [01:34<01:24,  2.20it/s]\u001b[A\n",
      " 53%|█████▎    | 206/390 [01:34<01:22,  2.23it/s]\u001b[A\n",
      " 53%|█████▎    | 207/390 [01:35<01:20,  2.27it/s]\u001b[A\n",
      " 53%|█████▎    | 208/390 [01:35<01:19,  2.28it/s]\u001b[A\n",
      " 54%|█████▎    | 209/390 [01:36<01:19,  2.28it/s]\u001b[A\n",
      " 54%|█████▍    | 210/390 [01:36<01:18,  2.29it/s]\u001b[A\n",
      " 54%|█████▍    | 211/390 [01:37<01:19,  2.25it/s]\u001b[A\n",
      " 54%|█████▍    | 212/390 [01:37<01:18,  2.26it/s]\u001b[A\n",
      " 55%|█████▍    | 213/390 [01:38<01:17,  2.27it/s]\u001b[A\n",
      " 55%|█████▍    | 214/390 [01:38<01:17,  2.27it/s]\u001b[A\n",
      " 55%|█████▌    | 215/390 [01:38<01:16,  2.28it/s]\u001b[A\n",
      " 55%|█████▌    | 216/390 [01:39<01:20,  2.16it/s]\u001b[A\n",
      " 56%|█████▌    | 217/390 [01:39<01:18,  2.20it/s]\u001b[A\n",
      " 56%|█████▌    | 218/390 [01:40<01:17,  2.22it/s]\u001b[A\n",
      " 56%|█████▌    | 219/390 [01:40<01:16,  2.22it/s]\u001b[A\n",
      " 56%|█████▋    | 220/390 [01:41<01:17,  2.21it/s]\u001b[A\n",
      " 57%|█████▋    | 221/390 [01:41<01:20,  2.09it/s]\u001b[A\n",
      " 57%|█████▋    | 222/390 [01:42<01:18,  2.15it/s]\u001b[A\n",
      " 57%|█████▋    | 223/390 [01:42<01:21,  2.05it/s]\u001b[A\n",
      " 57%|█████▋    | 224/390 [01:43<01:18,  2.12it/s]\u001b[A\n",
      " 58%|█████▊    | 225/390 [01:43<01:16,  2.17it/s]\u001b[A\n",
      " 58%|█████▊    | 226/390 [01:44<01:14,  2.21it/s]\u001b[A\n",
      " 58%|█████▊    | 227/390 [01:44<01:13,  2.23it/s]\u001b[A\n",
      " 58%|█████▊    | 228/390 [01:44<01:11,  2.25it/s]\u001b[A\n",
      " 59%|█████▊    | 229/390 [01:45<01:10,  2.27it/s]\u001b[A\n",
      " 59%|█████▉    | 230/390 [01:45<01:09,  2.29it/s]\u001b[A\n",
      " 59%|█████▉    | 231/390 [01:46<01:09,  2.29it/s]\u001b[A\n",
      " 59%|█████▉    | 232/390 [01:46<01:08,  2.30it/s]\u001b[A\n",
      " 60%|█████▉    | 233/390 [01:47<01:08,  2.31it/s]\u001b[A\n",
      " 60%|██████    | 234/390 [01:47<01:07,  2.32it/s]\u001b[A\n",
      " 60%|██████    | 235/390 [01:47<01:06,  2.32it/s]\u001b[A\n",
      " 61%|██████    | 236/390 [01:48<01:06,  2.33it/s]\u001b[A\n",
      " 61%|██████    | 237/390 [01:48<01:09,  2.19it/s]\u001b[A\n",
      " 61%|██████    | 238/390 [01:49<01:09,  2.20it/s]\u001b[A\n",
      " 61%|██████▏   | 239/390 [01:49<01:12,  2.08it/s]\u001b[A\n",
      " 62%|██████▏   | 240/390 [01:50<01:09,  2.15it/s]\u001b[A\n",
      " 62%|██████▏   | 241/390 [01:50<01:07,  2.21it/s]\u001b[A\n",
      " 62%|██████▏   | 242/390 [01:51<01:05,  2.24it/s]\u001b[A\n",
      " 62%|██████▏   | 243/390 [01:51<01:04,  2.27it/s]\u001b[A\n",
      " 63%|██████▎   | 244/390 [01:52<01:07,  2.16it/s]\u001b[A\n",
      " 63%|██████▎   | 245/390 [01:52<01:05,  2.20it/s]\u001b[A\n",
      " 63%|██████▎   | 246/390 [01:53<01:08,  2.11it/s]\u001b[A\n",
      " 63%|██████▎   | 247/390 [01:53<01:06,  2.15it/s]\u001b[A\n",
      " 64%|██████▎   | 248/390 [01:54<01:12,  1.96it/s]\u001b[A\n",
      " 64%|██████▍   | 249/390 [01:54<01:08,  2.06it/s]\u001b[A\n",
      " 64%|██████▍   | 250/390 [01:54<01:07,  2.07it/s]\u001b[A\n",
      " 64%|██████▍   | 251/390 [01:55<01:04,  2.15it/s]\u001b[A\n",
      " 65%|██████▍   | 252/390 [01:55<01:02,  2.20it/s]\u001b[A\n",
      " 65%|██████▍   | 253/390 [01:56<01:01,  2.24it/s]\u001b[A\n",
      " 65%|██████▌   | 254/390 [01:56<00:59,  2.27it/s]\u001b[A\n",
      " 65%|██████▌   | 255/390 [01:57<00:59,  2.28it/s]\u001b[A\n",
      " 66%|██████▌   | 256/390 [01:57<00:58,  2.30it/s]\u001b[A\n",
      " 66%|██████▌   | 257/390 [01:57<00:57,  2.30it/s]\u001b[A\n",
      " 66%|██████▌   | 258/390 [01:58<00:58,  2.27it/s]\u001b[A\n",
      " 66%|██████▋   | 259/390 [01:58<00:57,  2.27it/s]\u001b[A\n",
      " 67%|██████▋   | 260/390 [01:59<00:56,  2.29it/s]\u001b[A\n",
      " 67%|██████▋   | 261/390 [01:59<00:55,  2.31it/s]\u001b[A\n",
      " 67%|██████▋   | 262/390 [02:00<00:55,  2.32it/s]\u001b[A\n",
      " 67%|██████▋   | 263/390 [02:00<00:54,  2.32it/s]\u001b[A\n",
      " 68%|██████▊   | 264/390 [02:01<00:54,  2.33it/s]\u001b[A\n",
      " 68%|██████▊   | 265/390 [02:01<00:58,  2.13it/s]\u001b[A\n",
      " 68%|██████▊   | 266/390 [02:02<00:57,  2.17it/s]\u001b[A\n",
      " 68%|██████▊   | 267/390 [02:02<01:01,  1.99it/s]\u001b[A\n",
      " 69%|██████▊   | 268/390 [02:03<00:58,  2.08it/s]\u001b[A\n",
      " 69%|██████▉   | 269/390 [02:03<00:57,  2.11it/s]\u001b[A\n",
      " 69%|██████▉   | 270/390 [02:03<00:55,  2.16it/s]\u001b[A\n",
      " 69%|██████▉   | 271/390 [02:04<00:53,  2.21it/s]\u001b[A\n",
      " 70%|██████▉   | 272/390 [02:04<00:52,  2.24it/s]\u001b[A\n",
      " 70%|███████   | 273/390 [02:05<00:51,  2.26it/s]\u001b[A\n",
      " 70%|███████   | 274/390 [02:05<00:51,  2.27it/s]\u001b[A\n",
      " 71%|███████   | 275/390 [02:06<00:50,  2.27it/s]\u001b[A\n",
      " 71%|███████   | 276/390 [02:06<00:49,  2.28it/s]\u001b[A\n",
      " 71%|███████   | 277/390 [02:06<00:49,  2.30it/s]\u001b[A\n",
      " 71%|███████▏  | 278/390 [02:07<00:48,  2.30it/s]\u001b[A\n",
      " 72%|███████▏  | 279/390 [02:07<00:48,  2.31it/s]\u001b[A\n",
      " 72%|███████▏  | 280/390 [02:08<00:50,  2.17it/s]\u001b[A\n",
      " 72%|███████▏  | 281/390 [02:08<00:49,  2.21it/s]\u001b[A\n",
      " 72%|███████▏  | 282/390 [02:09<00:48,  2.23it/s]\u001b[A\n",
      " 73%|███████▎  | 283/390 [02:09<00:50,  2.13it/s]\u001b[A\n",
      " 73%|███████▎  | 284/390 [02:10<00:49,  2.15it/s]\u001b[A\n",
      " 73%|███████▎  | 285/390 [02:10<00:47,  2.19it/s]\u001b[A\n",
      " 73%|███████▎  | 286/390 [02:11<00:46,  2.22it/s]\u001b[A\n",
      " 74%|███████▎  | 287/390 [02:11<00:45,  2.24it/s]\u001b[A\n",
      " 74%|███████▍  | 288/390 [02:11<00:45,  2.26it/s]\u001b[A\n",
      " 74%|███████▍  | 289/390 [02:12<00:44,  2.27it/s]\u001b[A\n",
      " 74%|███████▍  | 290/390 [02:12<00:43,  2.28it/s]\u001b[A\n",
      " 75%|███████▍  | 291/390 [02:13<00:43,  2.28it/s]\u001b[A\n",
      " 75%|███████▍  | 292/390 [02:13<00:42,  2.29it/s]\u001b[A\n",
      " 75%|███████▌  | 293/390 [02:14<00:42,  2.30it/s]\u001b[A\n",
      " 75%|███████▌  | 294/390 [02:14<00:42,  2.28it/s]\u001b[A\n",
      " 76%|███████▌  | 295/390 [02:15<00:41,  2.28it/s]\u001b[A\n",
      " 76%|███████▌  | 296/390 [02:15<00:40,  2.30it/s]\u001b[A\n",
      " 76%|███████▌  | 297/390 [02:15<00:40,  2.31it/s]\u001b[A\n",
      " 76%|███████▋  | 298/390 [02:16<00:39,  2.32it/s]\u001b[A\n",
      " 77%|███████▋  | 299/390 [02:16<00:41,  2.18it/s]\u001b[A\n",
      " 77%|███████▋  | 300/390 [02:17<00:41,  2.19it/s]\u001b[A\n",
      " 77%|███████▋  | 301/390 [02:17<00:40,  2.20it/s]\u001b[A\n",
      " 77%|███████▋  | 302/390 [02:18<00:39,  2.20it/s]\u001b[A\n",
      " 78%|███████▊  | 303/390 [02:18<00:38,  2.23it/s]\u001b[A\n",
      " 78%|███████▊  | 304/390 [02:19<00:38,  2.26it/s]\u001b[A\n",
      " 78%|███████▊  | 305/390 [02:19<00:37,  2.27it/s]\u001b[A\n",
      " 78%|███████▊  | 306/390 [02:19<00:36,  2.29it/s]\u001b[A\n",
      " 79%|███████▊  | 307/390 [02:20<00:36,  2.29it/s]\u001b[A\n",
      " 79%|███████▉  | 308/390 [02:20<00:35,  2.29it/s]\u001b[A\n",
      " 79%|███████▉  | 309/390 [02:21<00:36,  2.21it/s]\u001b[A\n",
      " 79%|███████▉  | 310/390 [02:21<00:35,  2.26it/s]\u001b[A\n",
      " 80%|███████▉  | 311/390 [02:22<00:34,  2.28it/s]\u001b[A\n",
      " 80%|████████  | 312/390 [02:22<00:34,  2.28it/s]\u001b[A\n",
      " 80%|████████  | 313/390 [02:22<00:33,  2.30it/s]\u001b[A\n",
      " 81%|████████  | 314/390 [02:23<00:33,  2.28it/s]\u001b[A\n",
      " 81%|████████  | 315/390 [02:23<00:32,  2.29it/s]\u001b[A\n",
      " 81%|████████  | 316/390 [02:24<00:32,  2.29it/s]\u001b[A\n",
      " 81%|████████▏ | 317/390 [02:24<00:31,  2.30it/s]\u001b[A\n",
      " 82%|████████▏ | 318/390 [02:25<00:31,  2.30it/s]\u001b[A\n",
      " 82%|████████▏ | 319/390 [02:25<00:31,  2.29it/s]\u001b[A\n",
      " 82%|████████▏ | 320/390 [02:26<00:30,  2.28it/s]\u001b[A\n",
      " 82%|████████▏ | 321/390 [02:26<00:30,  2.29it/s]\u001b[A\n",
      " 83%|████████▎ | 322/390 [02:26<00:29,  2.29it/s]\u001b[A\n",
      " 83%|████████▎ | 323/390 [02:27<00:29,  2.30it/s]\u001b[A\n",
      " 83%|████████▎ | 324/390 [02:27<00:30,  2.16it/s]\u001b[A\n",
      " 83%|████████▎ | 325/390 [02:28<00:29,  2.20it/s]\u001b[A\n",
      " 84%|████████▎ | 326/390 [02:28<00:28,  2.23it/s]\u001b[A\n",
      " 84%|████████▍ | 327/390 [02:29<00:28,  2.23it/s]\u001b[A\n",
      " 84%|████████▍ | 328/390 [02:29<00:28,  2.21it/s]\u001b[A\n",
      " 84%|████████▍ | 329/390 [02:30<00:27,  2.22it/s]\u001b[A\n",
      " 85%|████████▍ | 330/390 [02:30<00:26,  2.26it/s]\u001b[A\n",
      " 85%|████████▍ | 331/390 [02:30<00:26,  2.27it/s]\u001b[A\n",
      " 85%|████████▌ | 332/390 [02:31<00:25,  2.26it/s]\u001b[A\n",
      " 85%|████████▌ | 333/390 [02:31<00:25,  2.28it/s]\u001b[A\n",
      " 86%|████████▌ | 334/390 [02:32<00:24,  2.28it/s]\u001b[A\n",
      " 86%|████████▌ | 335/390 [02:32<00:24,  2.29it/s]\u001b[A\n",
      " 86%|████████▌ | 336/390 [02:33<00:23,  2.29it/s]\u001b[A\n",
      " 86%|████████▋ | 337/390 [02:33<00:24,  2.14it/s]\u001b[A\n",
      " 87%|████████▋ | 338/390 [02:34<00:24,  2.16it/s]\u001b[A\n",
      " 87%|████████▋ | 339/390 [02:34<00:24,  2.08it/s]\u001b[A\n",
      " 87%|████████▋ | 340/390 [02:35<00:23,  2.14it/s]\u001b[A\n",
      " 87%|████████▋ | 341/390 [02:35<00:22,  2.18it/s]\u001b[A\n",
      " 88%|████████▊ | 342/390 [02:35<00:21,  2.22it/s]\u001b[A\n",
      " 88%|████████▊ | 343/390 [02:36<00:20,  2.25it/s]\u001b[A\n",
      " 88%|████████▊ | 344/390 [02:36<00:20,  2.27it/s]\u001b[A\n",
      " 88%|████████▊ | 345/390 [02:37<00:19,  2.28it/s]\u001b[A\n",
      " 89%|████████▊ | 346/390 [02:37<00:19,  2.28it/s]\u001b[A\n",
      " 89%|████████▉ | 347/390 [02:38<00:18,  2.29it/s]\u001b[A\n",
      " 89%|████████▉ | 348/390 [02:38<00:18,  2.30it/s]\u001b[A\n",
      " 89%|████████▉ | 349/390 [02:38<00:17,  2.31it/s]\u001b[A\n",
      " 90%|████████▉ | 350/390 [02:39<00:18,  2.17it/s]\u001b[A\n",
      " 90%|█████████ | 351/390 [02:39<00:17,  2.21it/s]\u001b[A\n",
      " 90%|█████████ | 352/390 [02:40<00:17,  2.11it/s]\u001b[A\n",
      " 91%|█████████ | 353/390 [02:40<00:17,  2.17it/s]\u001b[A\n",
      " 91%|█████████ | 354/390 [02:41<00:17,  2.12it/s]\u001b[A\n",
      " 91%|█████████ | 355/390 [02:41<00:16,  2.17it/s]\u001b[A\n",
      " 91%|█████████▏| 356/390 [02:42<00:15,  2.20it/s]\u001b[A\n",
      " 92%|█████████▏| 357/390 [02:42<00:15,  2.20it/s]\u001b[A\n",
      " 92%|█████████▏| 358/390 [02:43<00:14,  2.23it/s]\u001b[A\n",
      " 92%|█████████▏| 359/390 [02:43<00:13,  2.25it/s]\u001b[A\n",
      " 92%|█████████▏| 360/390 [02:44<00:13,  2.27it/s]\u001b[A\n",
      " 93%|█████████▎| 361/390 [02:44<00:12,  2.28it/s]\u001b[A\n",
      " 93%|█████████▎| 362/390 [02:44<00:12,  2.28it/s]\u001b[A\n",
      " 93%|█████████▎| 363/390 [02:45<00:11,  2.29it/s]\u001b[A\n",
      " 93%|█████████▎| 364/390 [02:45<00:11,  2.30it/s]\u001b[A\n",
      " 94%|█████████▎| 365/390 [02:46<00:10,  2.30it/s]\u001b[A\n",
      " 94%|█████████▍| 366/390 [02:46<00:11,  2.18it/s]\u001b[A\n",
      " 94%|█████████▍| 367/390 [02:47<00:10,  2.22it/s]\u001b[A\n",
      " 94%|█████████▍| 368/390 [02:47<00:09,  2.26it/s]\u001b[A\n",
      " 95%|█████████▍| 369/390 [02:48<00:09,  2.16it/s]\u001b[A\n",
      " 95%|█████████▍| 370/390 [02:48<00:09,  2.21it/s]\u001b[A\n",
      " 95%|█████████▌| 371/390 [02:48<00:08,  2.24it/s]\u001b[A\n",
      " 95%|█████████▌| 372/390 [02:49<00:07,  2.27it/s]\u001b[A\n",
      " 96%|█████████▌| 373/390 [02:49<00:07,  2.29it/s]\u001b[A\n",
      " 96%|█████████▌| 374/390 [02:50<00:07,  2.12it/s]\u001b[A\n",
      " 96%|█████████▌| 375/390 [02:50<00:06,  2.16it/s]\u001b[A\n",
      " 96%|█████████▋| 376/390 [02:51<00:06,  2.08it/s]\u001b[A\n",
      " 97%|█████████▋| 377/390 [02:51<00:06,  2.13it/s]\u001b[A\n",
      " 97%|█████████▋| 378/390 [02:52<00:05,  2.07it/s]\u001b[A\n",
      " 97%|█████████▋| 379/390 [02:52<00:05,  2.14it/s]\u001b[A\n",
      " 97%|█████████▋| 380/390 [02:53<00:04,  2.19it/s]\u001b[A\n",
      " 98%|█████████▊| 381/390 [02:53<00:04,  2.21it/s]\u001b[A\n",
      " 98%|█████████▊| 382/390 [02:54<00:03,  2.24it/s]\u001b[A\n",
      " 98%|█████████▊| 383/390 [02:54<00:03,  2.14it/s]\u001b[A\n",
      " 98%|█████████▊| 384/390 [02:54<00:02,  2.20it/s]\u001b[A\n",
      " 99%|█████████▊| 385/390 [02:55<00:02,  2.24it/s]\u001b[A\n",
      " 99%|█████████▉| 386/390 [02:55<00:01,  2.27it/s]\u001b[A\n",
      " 99%|█████████▉| 387/390 [02:56<00:01,  2.27it/s]\u001b[A\n",
      " 99%|█████████▉| 388/390 [02:56<00:00,  2.28it/s]\u001b[A\n",
      "100%|█████████▉| 389/390 [02:57<00:00,  2.31it/s]\u001b[A\n",
      "100%|██████████| 390/390 [02:57<00:00,  2.32it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [03:19<1:19:49, 199.57s/it]"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f81728631c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    352\u001b[0m             create_adversarial_video(video_path, model_path, model_type, output_path,\n\u001b[1;32m    353\u001b[0m                             \u001b[0mstart_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"iterative_fgsm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                             compress = True, cuda=True, showlabel = True)\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0menablePrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mpbar_global\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f81728631c66>\u001b[0m in \u001b[0;36mcreate_adversarial_video\u001b[0;34m(video_path, model_path, model_type, output_path, start_frame, end_frame, attack, compress, cuda, showlabel)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mframe_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mstart_frame\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_frames\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mend_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_frame\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mend_frame\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_frame\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
    "    \"\"\"\n",
    "    Expects a dlib face to generate a quadratic bounding box.\n",
    "    :param face: dlib face class\n",
    "    :param width: frame width\n",
    "    :param height: frame height\n",
    "    :param scale: bounding box size multiplier to get a bigger face region\n",
    "    :param minsize: set minimum bounding box size\n",
    "    :return: x, y, bounding_box_size in opencv form\n",
    "    \"\"\"\n",
    "    x1 = face.left()\n",
    "    y1 = face.top()\n",
    "    x2 = face.right()\n",
    "    y2 = face.bottom()\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
    "    if minsize:\n",
    "        if size_bb < minsize:\n",
    "            size_bb = minsize\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "    # Check for out of bounds, x-y top left corner\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    # Check for too big bb size for given x, y\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "\n",
    "    return x1, y1, size_bb\n",
    "# Preprocessamento\n",
    "\n",
    "def preprocess_image(image, model_type, cuda=True, legacy = False):\n",
    "    \"\"\"\n",
    "    Preprocesses the image such that it can be fed into our network.\n",
    "    During this process we envoke PIL to cast it into a PIL image.\n",
    "\n",
    "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
    "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
    "    necessarily casted to cuda\n",
    "    \"\"\"\n",
    "    # Revert from BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Preprocess using the preprocessing function used during training and\n",
    "    # casting it to PIL image\n",
    "    if not legacy:\n",
    "        # only conver to tensor here, \n",
    "        # other transforms -> resize, normalize differentiable done in predict_from_model func\n",
    "        # same for meso, xception\n",
    "        preprocess = xception_default_data_transforms['to_tensor']\n",
    "    else:\n",
    "        if model_type == \"xception\":\n",
    "            preprocess = xception_default_data_transforms['test']\n",
    "        elif model_type == \"meso\":\n",
    "            preprocess = mesonet_default_data_transforms['test']\n",
    "\n",
    "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
    "    \n",
    "    # Add first dimension as the network expects a batch\n",
    "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
    "    if cuda:\n",
    "        preprocessed_image = preprocessed_image.cuda()\n",
    "\n",
    "    preprocessed_image.requires_grad = True\n",
    "    return preprocessed_image\n",
    "\n",
    "def un_preprocess_image(image, size):\n",
    "    \"\"\"\n",
    "    Tensor to PIL image and RGB to BGR\n",
    "    \"\"\"\n",
    "    \n",
    "    image.detach()\n",
    "    new_image = image.squeeze(0)\n",
    "    new_image = new_image.detach().cpu()\n",
    "\n",
    "    undo_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    new_image = undo_transform(new_image)\n",
    "    new_image = numpy.array(new_image)\n",
    "\n",
    "    new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return new_image\n",
    "    \n",
    "def predict_with_model_legacy(image, model, model_type, post_function=nn.Softmax(dim=1),\n",
    "                       cuda=True):\n",
    "    \"\"\"\n",
    "    Predicts the label of an input image. Preprocesses the input image and\n",
    "    casts it to cuda if required\n",
    "\n",
    "    :param image: numpy image\n",
    "    :param model: torch model with linear layer at the end\n",
    "    :param post_function: e.g., softmax\n",
    "    :param cuda: enables cuda, must be the same parameter as the model\n",
    "    :return: prediction (1 = fake, 0 = real)\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    preprocessed_image = preprocess_image(image, model_type, cuda, legacy = True)\n",
    "\n",
    "    # Model prediction\n",
    "    output = model(preprocessed_image)\n",
    "    output = post_function(output)\n",
    "\n",
    "    # Cast to desired\n",
    "    _, prediction = torch.max(output, 1)    # argmax\n",
    "    prediction = float(prediction.cpu().numpy())\n",
    "\n",
    "    return int(prediction), output\n",
    "\n",
    "def create_adversarial_video(video_path, model_path, model_type, output_path,\n",
    "                            start_frame=0, end_frame=None, attack=\"iterative_fgsm\", \n",
    "                            compress = True, cuda=True, showlabel = True):\n",
    "    \"\"\"\n",
    "    Reads a video and evaluates a subset of frames with the a detection network\n",
    "    that takes in a full frame. Outputs are only given if a face is present\n",
    "    and the face is highlighted using dlib.\n",
    "    :param video_path: path to video file\n",
    "    :param model_path: path to model file (should expect the full sized image)\n",
    "    :param output_path: path where the output video is stored\n",
    "    :param start_frame: first frame to evaluate\n",
    "    :param end_frame: last frame to evaluate\n",
    "    :param cuda: enable cuda\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('Starting: {}'.format(video_path))\n",
    "\n",
    "    # Read and write\n",
    "    reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    if compress:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    else:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'HFYU') # Chnaged to HFYU because it is lossless\n",
    "\n",
    "    fps = reader.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    writer = None\n",
    "\n",
    "    # Face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # Load model\n",
    "    if model_path is not None:\n",
    "        if not cuda:\n",
    "            model = torch.load(model_path, map_location = \"cpu\")\n",
    "        else:\n",
    "            model = torch.load(model_path)\n",
    "        print('Model found in {}'.format(model_path))\n",
    "    else:\n",
    "        print('No model found, initializing random model.')\n",
    "    if cuda:\n",
    "        print(\"Converting mode to cuda\")\n",
    "        model = model.cuda()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Converted to cuda\")\n",
    "\n",
    "    # raise Exception()\n",
    "    # Text variables\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    thickness = 2\n",
    "    font_scale = 1\n",
    "\n",
    "    # Frame numbers and length of output video\n",
    "    frame_num = 0\n",
    "    print(start_frame,num_frames)\n",
    "    assert start_frame < num_frames - 1\n",
    "    end_frame = end_frame if end_frame else num_frames\n",
    "    pbar = tqdm(total=end_frame-start_frame)\n",
    "\n",
    "    metrics = {\n",
    "        'total_fake_frames' : 0,\n",
    "        'total_real_frames' : 0,\n",
    "        'total_frames' : 0,\n",
    "        'percent_fake_frames' : 0,\n",
    "        'probs_list' : [],\n",
    "        'attack_meta_data' : [],\n",
    "    }\n",
    "\n",
    "    while reader.isOpened():\n",
    "        _, image = reader.read()\n",
    "        if image is None:\n",
    "            break\n",
    "        frame_num += 1\n",
    "\n",
    "        if frame_num < start_frame:\n",
    "            continue\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Image size\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Init output writer\n",
    "        if writer is None:\n",
    "            writer = cv2.VideoWriter(join(output_path, video_fn), fourcc, fps,\n",
    "                                     (height, width)[::-1])\n",
    "\n",
    "            # writer = cv2.VideoWriter(join(output_path, video_fn), 0, 1,\n",
    "            #                          (height, width)[::-1])\n",
    "\n",
    "        # 2. Detect with dlib\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector(gray, 1)\n",
    "        if len(faces):\n",
    "            # For now only take biggest face\n",
    "            face = faces[0]\n",
    "\n",
    "            # --- Prediction ---------------------------------------------------\n",
    "            # Face crop with dlib and bounding box scale enlargement\n",
    "            x, y, size = get_boundingbox(face, width, height)\n",
    "            cropped_face = image[y:y+size, x:x+size]\n",
    "\n",
    "            \n",
    "            processed_image = preprocess_image(cropped_face, model_type, cuda = cuda)\n",
    "            \n",
    "            # Attack happening here\n",
    "\n",
    "            # white-box attacks\n",
    "            if attack == \"iterative_fgsm\":\n",
    "                perturbed_image, attack_meta_data = attack_algos.iterative_fgsm(processed_image, model, model_type, cuda)\n",
    "            elif attack == \"robust\":\n",
    "                perturbed_image, attack_meta_data = attack_algos.robust_fgsm(processed_image, model, model_type, cuda)\n",
    "            elif attack == \"carlini_wagner\":\n",
    "                perturbed_image, attack_meta_data = attack_algos.carlini_wagner_attack(processed_image, model_type, model, cuda)\n",
    "\n",
    "            # black-box attacks\n",
    "            elif attack == \"black_box\":\n",
    "                perturbed_image, attack_meta_data = attack_algos.black_box_attack(processed_image, model, model_type, \n",
    "                    cuda, transform_set={}, desired_acc = 0.999999)\n",
    "            elif attack == \"black_box_robust\":\n",
    "                perturbed_image, attack_meta_data = attack_algos.black_box_attack(processed_image, model, \n",
    "                    model_type, cuda, transform_set = {\"gauss_blur\", \"translation\", \"resize\"})\n",
    "            \n",
    "            # Undo the processing of xceptionnet, mesonet\n",
    "            unpreprocessed_image = un_preprocess_image(perturbed_image, size)\n",
    "            image[y:y+size, x:x+size] = unpreprocessed_image\n",
    "            \n",
    "\n",
    "            cropped_face = image[y:y+size, x:x+size]\n",
    "            processed_image = preprocess_image(cropped_face, model_type, cuda = cuda)\n",
    "            prediction, output, logits = attack_algos.predict_with_model(processed_image, model, model_type, cuda=cuda)\n",
    "\n",
    "            print (\">>>>Prediction for frame no. {}: {}\".format(frame_num ,output))\n",
    "\n",
    "            prediction, output = predict_with_model_legacy(cropped_face, model, model_type, cuda=cuda)\n",
    "\n",
    "            print (\">>>>Prediction LEGACY for frame no. {}: {}\".format(frame_num ,output))\n",
    "\n",
    "            label = 'fake' if prediction == 1 else 'real'\n",
    "            if label == 'fake':\n",
    "                metrics['total_fake_frames'] += 1.\n",
    "            else:\n",
    "                metrics['total_real_frames'] += 1.\n",
    "\n",
    "            metrics['total_frames'] += 1.\n",
    "            metrics['probs_list'].append(output[0].detach().cpu().numpy().tolist())\n",
    "            metrics['attack_meta_data'].append(attack_meta_data)\n",
    "\n",
    "            if showlabel:\n",
    "                # Text and bb\n",
    "                # print a bounding box in the generated video\n",
    "                x = face.left()\n",
    "                y = face.top()\n",
    "                w = face.right() - x\n",
    "                h = face.bottom() - y\n",
    "                label = 'fake' if prediction == 1 else 'real'\n",
    "                color = (0, 255, 0) if prediction == 0 else (0, 0, 255)\n",
    "                output_list = ['{0:.2f}'.format(float(x)) for x in\n",
    "                               output.detach().cpu().numpy()[0]]\n",
    "\n",
    "                cv2.putText(image, str(output_list)+'=>'+label, (x, y+h+30),\n",
    "                            font_face, font_scale,\n",
    "                            color, thickness, 2)\n",
    "                # draw box over face\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        if frame_num >= end_frame:\n",
    "            break\n",
    "\n",
    "        writer.write(image)\n",
    "    pbar.close()\n",
    "\n",
    "    metrics['percent_fake_frames'] = metrics['total_fake_frames']/metrics['total_frames']\n",
    "\n",
    "    with open(join(output_path, video_fn.replace(\".avi\", \"_metrics_attack.json\")), \"w\") as f:\n",
    "        f.write(json.dumps(metrics))\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "        print('Finished! Output saved under {}'.format(output_path))\n",
    "    else:\n",
    "        print('Input video file was empty')\n",
    "\n",
    "\n",
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     p = argparse.ArgumentParser(\n",
    "#         formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "#     p.add_argument('--video_path', '-i', type=str)\n",
    "#     p.add_argument('--model_path', '-mi', type=str, default=None)\n",
    "#     p.add_argument('--model_type', '-mt', type=str, default=\"xception\")\n",
    "#     p.add_argument('--output_path', '-o', type=str,\n",
    "#                    default='.')\n",
    "#     p.add_argument('--start_frame', type=int, default=0)\n",
    "#     p.add_argument('--end_frame', type=int, default=None)\n",
    "#     p.add_argument('--attack', '-a', type=str, default=\"iterative_fgsm\")\n",
    "#     p.add_argument('--compress', action='store_true')\n",
    "#     p.add_argument('--cuda', action='store_true')\n",
    "#     p.add_argument('--showlabel', action='store_true') # add face labels in the generated video\n",
    "\n",
    "#     args = p.parse_args()\n",
    "\n",
    "#     video_path = args.video_path\n",
    "\n",
    "#     video_path = '/root/deepfake-framework/1-AdversarialDeepFakes/videos/source/'\n",
    "\n",
    "    video_path = '/home/ec2-user/SageMaker/deepfake-framework/Dataset/25/manipulated_sequences/Deepfakes/raw/videos'\n",
    "    model_path = '/home/ec2-user/SageMaker/deepfake-framework/1-AdversarialDeepFakes/all_raw.p'\n",
    "    model_type = \"xception\"\n",
    "    output_path = '/home/ec2-user/SageMaker/deepfake-framework/Dataset/25/manipulated_sequences/Deepfakes/raw/2-Adv_attacked_videos'\n",
    "    \n",
    "    start_frame = 0\n",
    "    end_frame = None\n",
    "    \n",
    "#     compress = 'store_true'\n",
    "    attack = 'iterative_fgsm' \n",
    "    cuda = 'store_true'\n",
    "#     showlabel = 'store_true'\n",
    "    \n",
    "    if video_path.endswith('.mp4') or video_path.endswith('.avi'):\n",
    "        create_adversarial_video(video_path, model_path, model_type, output_path,\n",
    "                            start_frame=0, end_frame=None, attack=\"iterative_fgsm\", \n",
    "                            compress = False, cuda=True, showlabel = False)\n",
    "    else:\n",
    "\n",
    "        videos = os.listdir(video_path)\n",
    "        pbar_global = tqdm(total=len(videos))\n",
    "        for video in videos:\n",
    "            video_path = join(video_path, video)\n",
    "            print('Video:', video_path)\n",
    "            blockPrint()\n",
    "#             create_adversarial_video(**vars(args))\n",
    "            create_adversarial_video(video_path, model_path, model_type, output_path,\n",
    "                            start_frame=0, end_frame=None, attack=\"iterative_fgsm\", \n",
    "                            compress = True, cuda=True, showlabel = True)\n",
    "            enablePrint()\n",
    "            pbar_global.update(1)\n",
    "        pbar_global.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd videos/source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 81788\n",
      "drwxr-xr-x 7 root root     6144 Feb 21 22:32 .\n",
      "drwxr-xr-x 9 root root     6144 Feb 21 19:44 ..\n",
      "drwxr-xr-x 2 root root     6144 Feb 21 19:30 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 root root    36033 Feb 21 22:32 Attack.ipynb\n",
      "-rw-r--r-- 1 root root     1142 Feb 17 22:30 LICENSE-FaceForensics\n",
      "-rw-r--r-- 1 root root     7437 Feb 17 22:30 README.md\n",
      "drwxr-xr-x 2 root root     6144 Feb 17 22:30 __pycache__\n",
      "-rw-r--r-- 1 root root     5818 Feb 17 22:30 aggregate_stats.py\n",
      "-rw-r--r-- 1 root root 83555882 Feb 21 22:07 all_raw.p\n",
      "-rw-r--r-- 1 root root     4861 Feb 17 23:23 attack-env.txt\n",
      "-rw-r--r-- 1 root root    13087 Feb 17 22:30 attack.py\n",
      "-rw-r--r-- 1 root root     4861 Feb 18 00:12 attack.yml\n",
      "-rw-r--r-- 1 root root    13903 Feb 17 22:30 attack_algos.py\n",
      "-rw-r--r-- 1 root root     4829 Feb 17 22:30 convert_to_mjpeg.py\n",
      "-rw-r--r-- 1 root root     1959 Feb 17 22:30 create_test_data.py\n",
      "drwxr-xr-x 3 root root     6144 Feb 17 22:30 dataset\n",
      "-rw-r--r-- 1 root root    10011 Feb 17 22:30 detect_from_video.py\n",
      "drwxr-xr-x 3 root root     6144 Feb 17 22:30 network\n",
      "-rw-r--r-- 1 root root      532 Feb 21 19:44 requirements.txt\n",
      "-rw-r--r-- 1 root root     2534 Feb 17 22:30 robust_transforms.py\n",
      "-rw-r--r-- 1 root root     5919 Feb 17 22:30 run_experiments.py\n",
      "-rw-r--r-- 1 root root     2102 Feb 17 22:30 test_split.json\n",
      "-rw-r--r-- 1 root root     6078 Feb 17 22:30 test_transforms.py\n",
      "-rw-r--r-- 1 root root     2294 Feb 17 22:30 video_compression.py\n",
      "drwxr-xr-x 4 root root     6144 Feb 21 22:15 videos\n"
     ]
    }
   ],
   "source": [
    "!ls -al "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 21 22:18:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   31C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
